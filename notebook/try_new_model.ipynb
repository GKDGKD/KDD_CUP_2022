{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json, time, os\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from utils import get_gnn_data\n",
    "from log.logutli import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 134, 288, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 134, 288, 10)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 17:32:09-main-INFO: LOCAL TIME: 2024_01_08_17_32_09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_name: Run_2024_01_08_17_32_09.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 17:32:14-main-INFO: Raw data.shape: (4727520, 13)\n",
      "2024-01-08 17:32:15-main-INFO: data.shape: (134, 10, 35280), means.shape: (10,), stds.shape: (10,)\n",
      "2024-01-08 17:32:15-main-INFO: train_original_data: (134, 10, 28800), \n",
      "val_original_data: (134, 10, 2880), \n",
      "test_original_data: (134, 10, 3600).\n"
     ]
    }
   ],
   "source": [
    "with open('../config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "config['data_path'] = '../data/'\n",
    "config[\"location_path\"] = \"../data/sdwpf_baidukddcup2022_turb_location.CSV\"\n",
    "\n",
    "\n",
    "# \"data_path\"      : \"./data/\",\n",
    "#     \"filename\"       : \"wtbdata_245days.csv\",\n",
    "#     \"location_path\"  : \"./data/sdwpf_baidukddcup2022_turb_location.CSV\",\n",
    "\n",
    "start_time   = time.time()\n",
    "current_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime(start_time))\n",
    "save_dir     = os.path.join(\"../result\", current_time + f'_test')\n",
    "log_id       = 'main'\n",
    "log_name     = f'Run_{current_time}.log'\n",
    "log_level    = 'info'\n",
    "Logger_      = Logger(log_id, save_dir, log_name, log_level)\n",
    "logger       = Logger_.logger\n",
    "logger.info(f\"LOCAL TIME: {current_time}\")\n",
    "\n",
    "train_original_data, val_original_data, test_original_data, _, _, _ = get_gnn_data(config, logger)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape: (28800,)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "x1 = train_original_data[0, -1, :]\n",
    "print(f'x1.shape: {x1.shape}')\n",
    "input_time_step = 48\n",
    "forecast_steps = 24\n",
    "\n",
    "train, test = x1[:input_time_step], x1[input_time_step:input_time_step+forecast_steps]\n",
    "\n",
    "# train_size = 80\n",
    "# train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "# 训练ARIMA模型\n",
    "order = (2, 1, 2)  # ARIMA(p, d, q)参数，可以根据实际情况调整\n",
    "model = ARIMA(train, order=order)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# 多步向前预测\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "print(forecast.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.79396946 2.79439029 2.75946172 2.69734311 2.71969366 2.79457732\n",
      " 2.7460888  2.79417988 2.78728301 2.69851208 2.77968476 2.70279047\n",
      " 2.7347265  2.72114317 2.5437182  2.19223488 2.03687991 1.43818463\n",
      " 1.6110974  1.38177055 1.28972653 1.00132037 0.63980736 0.94687015]\n",
      "[2.7936841  2.79360479 2.79356603 2.79354963 2.79354252 2.79353946\n",
      " 2.79353814 2.79353757 2.79353732 2.79353722 2.79353717 2.79353715\n",
      " 2.79353714 2.79353714 2.79353714 2.79353714 2.79353714 2.79353714\n",
      " 2.79353714 2.79353714 2.79353714 2.79353714 2.79353714 2.79353714]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 10, 3600)\n",
      "(3600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3529 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3529/3529 [02:51<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3529/3529 [02:58<00:00, 19.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3529/3529 [03:00<00:00, 19.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_indices = [(i, i + (config['input_len'] + config['output_len'])) \n",
    "        for i in range(train_original_data.shape[2] - \\\n",
    "                        (config['input_len'] + config['output_len']) + 1)]\n",
    "val_indices = [(i, i + (config['input_len'] + config['output_len'])) \n",
    "        for i in range(val_original_data.shape[2] - \\\n",
    "                        (config['input_len'] + config['output_len']) + 1)]\n",
    "\n",
    "print(test_original_data.shape)\n",
    "\n",
    "\n",
    "for turb_id in range(test_original_data.shape[0]):  # [num_nodes, num_features, seq_len]\n",
    "    data = test_original_data[turb_id, -1, :]  # [seq_len, ]\n",
    "    print(data.shape)\n",
    "    for i in tqdm(range(test_original_data.shape[2] - \\\n",
    "                        (config['input_len'] + config['output_len']) + 1)):\n",
    "        train, test = data[i:i+input_time_step], data[i+input_time_step:i+input_time_step+forecast_steps]\n",
    "\n",
    "        # 训练ARIMA模型\n",
    "        order = (1, 1, 1)  # ARIMA(p, d, q)参数，可以根据实际情况调整\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # 多步向前预测\n",
    "        forecast = model_fit.forecast(steps=forecast_steps)\n",
    "        # print(forecast.shape)\n",
    "    if turb_id > 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3529/3529 [41:11<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "X = test_original_data[66, -1, :]\n",
    "for i in tqdm(range(len(X) - \\\n",
    "                    (config['input_len'] + config['output_len']) + 1)):\n",
    "    train, test = X[i:i+input_time_step], X[i+input_time_step:i+input_time_step+forecast_steps]\n",
    "\n",
    "    order = (1, 1, 1)  # ARIMA(p, d, q)参数，可以根据实际情况调整\n",
    "    model = ARIMA(X, order=order)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # 多步向前预测\n",
    "    forecast = model_fit.forecast(steps=forecast_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([64, 132, 10, 48])\n",
      "x2.shape: torch.Size([64, 132, 64, 48])\n",
      "x3.shape: torch.Size([64, 64, 132, 48])\n"
     ]
    }
   ],
   "source": [
    "batch_size       = 64\n",
    "num_of_timesteps = 48\n",
    "num_of_nodes     = 132\n",
    "num_of_features  = 10\n",
    "num_filters      = 64\n",
    "num_time_filter  = 64\n",
    "time_strides     = 1\n",
    "\n",
    "x = torch.randn(batch_size, num_of_nodes, num_of_features, num_of_timesteps)\n",
    "print(f'x.shape: {x.shape}')\n",
    "\n",
    "x2 = torch.randn(batch_size, num_of_nodes, num_filters, num_of_timesteps) \n",
    "print(f'x2.shape: {x2.shape}')\n",
    "\n",
    "\n",
    "# temporal_At = torch.randn(batch_size, num_of_timesteps, num_of_timesteps)\n",
    "time_conv = nn.Conv2d(num_filters, num_time_filter, kernel_size=(1, 3), stride=(1, time_strides), padding=(0, 1))\n",
    "x3 = time_conv(x2.permute(0, 2, 1, 3))  # [bs, n, f, t] -> [bs, f, n, t]\n",
    "print(f'x3.shape: {x3.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm \n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        其实这就是一个裁剪的模块，裁剪多出来的padding\n",
    "        \"\"\"\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        \"\"\"\n",
    "        相当于一个Residual block\n",
    "\n",
    "        :param n_inputs: int, 输入通道数\n",
    "        :param n_outputs: int, 输出通道数\n",
    "        :param kernel_size: int, 卷积核尺寸\n",
    "        :param stride: int, 步长，一般为1\n",
    "        :param dilation: int, 膨胀系数\n",
    "        :param padding: int, 填充系数\n",
    "        :param dropout: float, dropout比率\n",
    "        \"\"\"\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        # 经过conv1，输出的size其实是(Batch, input_channel, seq_len + padding)\n",
    "        self.chomp1   = Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu1    = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2   = Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len\n",
    "        self.relu2    = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(\n",
    "            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        参数初始化\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: size of (Batch, input_channel, seq_len)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx.shape: torch.Size([64, 10, 48])\n",
      "xx.shape: torch.Size([64, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "stride = 1\n",
    "dilation = 2\n",
    "\n",
    "xx = torch.randn(batch_size, num_of_features, num_of_timesteps)\n",
    "print(f'xx.shape: {xx.shape}')\n",
    "\n",
    "tcn_block = TemporalBlock(num_of_features, num_of_timesteps, kernel_size, stride, dilation, padding=(kernel_size - 1)*dilation)\n",
    "xx = tcn_block(xx)\n",
    "print(f'xx.shape: {xx.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, dilation: 1, in_channels: 10, out_channels: 25\n",
      "i: 1, dilation: 2, in_channels: 25, out_channels: 25\n",
      "i: 2, dilation: 4, in_channels: 25, out_channels: 25\n",
      "i: 3, dilation: 8, in_channels: 25, out_channels: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TemporalBlock(\n",
       "    (conv1): Conv1d(10, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (chomp1): Chomp1d()\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (conv2): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (chomp2): Chomp1d()\n",
       "    (relu2): ReLU()\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(10, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "      (1): Chomp1d()\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "      (5): Chomp1d()\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (downsample): Conv1d(10, 25, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (1): TemporalBlock(\n",
       "    (conv1): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (chomp1): Chomp1d()\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (conv2): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (chomp2): Chomp1d()\n",
       "    (relu2): ReLU()\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "      (1): Chomp1d()\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "      (5): Chomp1d()\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (2): TemporalBlock(\n",
       "    (conv1): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (chomp1): Chomp1d()\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (conv2): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (chomp2): Chomp1d()\n",
       "    (relu2): ReLU()\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "      (1): Chomp1d()\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "      (5): Chomp1d()\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (3): TemporalBlock(\n",
       "    (conv1): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "    (chomp1): Chomp1d()\n",
       "    (relu1): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (conv2): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "    (chomp2): Chomp1d()\n",
       "    (relu2): ReLU()\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "      (1): Chomp1d()\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Conv1d(25, 25, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "      (5): Chomp1d()\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = []\n",
    "num_channels = [25, 25, 25, 25]\n",
    "num_layers = len(num_channels)\n",
    "for i in range(num_layers):\n",
    "    dilation_size = 2 ** i\n",
    "    in_channels = num_of_features if i == 0 else num_channels[i - 1]\n",
    "    out_channels = num_channels[i]\n",
    "    print(f'i: {i}, dilation: {dilation_size}, in_channels: {in_channels}, out_channels: {out_channels}')\n",
    "    layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride, dilation_size, padding=(kernel_size - 1)*dilation_size)]\n",
    "\n",
    "net = nn.Sequential(*layers)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx.shape: torch.Size([64, 10, 48])\n",
      "xx2.shape: torch.Size([64, 25, 48])\n"
     ]
    }
   ],
   "source": [
    "xx = torch.randn(batch_size, num_of_features, num_of_timesteps)\n",
    "print(f'xx.shape: {xx.shape}')\n",
    "\n",
    "xx2 = net(xx)  \n",
    "print(f'xx2.shape: {xx2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1.shape: torch.Size([64, 10, 48])\n",
      "before chomp: a2.shape: torch.Size([64, 25, 50])\n",
      "a2.shape: torch.Size([64, 25, 48])\n",
      "a3.shape: torch.Size([64, 25, 48])\n"
     ]
    }
   ],
   "source": [
    "pad = kernel_size - 1\n",
    "chomp = Chomp1d(pad)\n",
    "relu = nn.ReLU()\n",
    "\n",
    "a1 = torch.randn(batch_size, num_of_features, num_of_timesteps)\n",
    "print(f'a1.shape: {a1.shape}')\n",
    "\n",
    "d1 = 1\n",
    "pad1 = (kernel_size-1)*d1\n",
    "chomp1 = Chomp1d(pad1)\n",
    "conv1 = nn.Conv1d(num_of_features, 25, kernel_size, stride=stride, padding=pad1, dilation=d1)\n",
    "a2 = conv1(a1)\n",
    "print(f'before chomp: a2.shape: {a2.shape}')\n",
    "a2 = chomp1(a2)\n",
    "a2 = relu(a2)\n",
    "print(f'a2.shape: {a2.shape}')\n",
    "\n",
    "d2 = 2\n",
    "pad2 = (kernel_size-1)*d2\n",
    "chomp2 = Chomp1d(pad2)\n",
    "conv2 = nn.Conv1d(25, 25, kernel_size, stride=stride, padding=(kernel_size-1)*d2, dilation=d2)\n",
    "a3 = conv2(a2)\n",
    "a3 = chomp2(a3)\n",
    "a3 = relu(a3)\n",
    "print(f'a3.shape: {a3.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3维TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp2d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp2d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        裁剪多余的padding。\n",
    "\n",
    "        Args:\n",
    "            x: [batch_size, num_nodes, num_features, num_time_steps]\n",
    "        \"\"\"\n",
    "        return x[:, :, :, : -self.chomp_size].contiguous()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([64, 132, 10, 48])\n",
      "a2.shape: torch.Size([64, 64, 132, 48])\n",
      "a3.shape: torch.Size([64, 64, 132, 48])\n"
     ]
    }
   ],
   "source": [
    "batch_size       = 64\n",
    "num_of_timesteps = 48\n",
    "num_of_nodes     = 132\n",
    "num_of_features  = 10\n",
    "num_filters      = 64\n",
    "num_time_filter  = 64\n",
    "time_strides     = 1\n",
    "kernel_size      = 3\n",
    "\n",
    "x = torch.randn(batch_size, num_of_nodes, num_of_features, num_of_timesteps)\n",
    "print(f'x.shape: {x.shape}')\n",
    "\n",
    "# x2 = torch.randn(batch_size, num_of_nodes, num_filters, num_of_timesteps) \n",
    "# print(f'x2.shape: {x2.shape}')\n",
    "\n",
    "\n",
    "# temporal_At = torch.randn(batch_size, num_of_timesteps, num_of_timesteps)\n",
    "# time_conv = nn.Conv2d(num_filters, num_time_filter, kernel_size=(1, kernel_size), stride=1, padding=(0, 1))\n",
    "# x3 = time_conv(x.permute(0, 2, 1, 3))  # [bs, n, f, t] -> [bs, f, n, t]\n",
    "# print(f'x3.shape: {x3.shape}')\n",
    "\n",
    "d1 = 1\n",
    "pad1 = (kernel_size-1)*d1\n",
    "chomp1 = Chomp2d(pad1)\n",
    "# conv1 = nn.Conv2d(num_of_features, 25, kernel_size, stride=stride, padding=pad1, dilation=d1)\n",
    "conv1 = nn.Conv2d(num_of_features, num_time_filter, kernel_size=(1, kernel_size), stride=1, dilation=d1, padding=(0, (kernel_size-1)*d1))\n",
    "a2 = conv1(x.permute(0, 2, 1, 3)) # [bs, n, f, t] -> [bs, f, n, t] -> [bs, f, n, t]\n",
    "a2 = chomp1(a2)\n",
    "a2 = relu(a2)\n",
    "print(f'a2.shape: {a2.shape}')\n",
    "\n",
    "d2 = 1\n",
    "pad2 = (kernel_size-1)*d2\n",
    "chomp2 = Chomp2d(pad2)\n",
    "conv2 = nn.Conv2d(num_time_filter, num_time_filter, kernel_size=(1, kernel_size), stride=1, padding=(0, (kernel_size-1)*d2), dilation=d2)\n",
    "a3 = conv2(a2)\n",
    "a3 = chomp2(a3)\n",
    "a3 = relu(a3)\n",
    "print(f'a3.shape: {a3.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock2d(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, dropout=0.2):\n",
    "        super(TemporalBlock2d, self).__init__()\n",
    "        padding  = (kernel_size - 1) * dilation\n",
    "        self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, kernel_size=(1, kernel_size),\n",
    "                                           stride=stride, padding=(0, padding), dilation=dilation))\n",
    "        self.chomp1   = Chomp2d(padding)\n",
    "        self.relu1    = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, kernel_size=(1, kernel_size),\n",
    "                                           stride=stride, padding=(0, padding), dilation=dilation))\n",
    "        self.chomp2   = Chomp2d(padding)\n",
    "        self.relu2    = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv2d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, num_nodes, num_features, num_time_steps]\n",
    "        \"\"\"\n",
    "        print(f'block: x.shape: {x.shape}')\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "    \n",
    "class TCN2d(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size: int = 10, \n",
    "                 output_size: int = 64,\n",
    "                 num_channels: list = [64, 128, 256], \n",
    "                 kernel_size: int = 3, \n",
    "                 dropout: float = 0.2,\n",
    "                 device: str = 'cpu'):\n",
    "        super(TCN2d, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i - 1]\n",
    "            out_channels = num_channels[i]\n",
    "            print(f'i: {i}, dilation: {dilation_size}, in_channels: {in_channels}, out_channels: {out_channels}')\n",
    "            layers += [TemporalBlock2d(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear  = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, num_nodes, num_features, num_time_steps]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)  # x: [bs, n, f, t] -> [bs, f, n, t]\n",
    "        print(f'tcn input: x.shape: {x.shape}')\n",
    "        x = x.to(self.device)\n",
    "        x = self.network(x)  # [bs, f, n, t] -> [bs, num_channels[-1], n, t]\n",
    "        print(f'tcn network output: x.shape: {x.shape}')\n",
    "        x = self.linear(x.permute(0, 2, 3, 1)) # [bs, num_channels[-1], n, t] -> [bs, n, t, num_channels[-1]] -> [bs, n, t, out_channels]\n",
    "        print(f'tcn linear output: x.shape: {x.shape}')\n",
    "        return x.permute(0, 3, 1, 2)  # [bs, n, t, out_channels] -> [bs, out_channels, n, t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([64, 132, 10, 48])\n",
      "block: x.shape: torch.Size([64, 10, 132, 48])\n",
      "x1.shape: torch.Size([64, 64, 132, 48])\n",
      "i: 0, dilation: 1, in_channels: 10, out_channels: 16\n",
      "i: 1, dilation: 2, in_channels: 16, out_channels: 32\n",
      "i: 2, dilation: 4, in_channels: 32, out_channels: 64\n",
      "tcn input: x.shape: torch.Size([64, 10, 132, 48])\n",
      "block: x.shape: torch.Size([64, 10, 132, 48])\n",
      "block: x.shape: torch.Size([64, 16, 132, 48])\n",
      "block: x.shape: torch.Size([64, 32, 132, 48])\n",
      "tcn network output: x.shape: torch.Size([64, 64, 132, 48])\n",
      "tcn linear output: x.shape: torch.Size([64, 132, 48, 64])\n",
      "x2.shape: torch.Size([64, 64, 132, 48])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, num_of_nodes, num_of_features, num_of_timesteps)\n",
    "print(f'x.shape: {x.shape}')\n",
    "\n",
    "block1 = TemporalBlock2d(num_of_features, num_time_filter, kernel_size, stride, dilation=1)\n",
    "x1 = block1(x.permute(0, 2, 1, 3)) # x: [bs, n, f, t] -> [bs, f, n, t] -> [bs, f, n, t]\n",
    "print(f'x1.shape: {x1.shape}')\n",
    "\n",
    "num_channels = [16, 32, 64]\n",
    "kernel_size = 3\n",
    "tcn = TCN2d(num_of_features, num_time_filter, num_channels, kernel_size)\n",
    "x2 = tcn(x)\n",
    "print(f'x2.shape: {x2.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
